# GPT-2 (small, 124M)

The list in this directory has been trained on the top 10 million plaintext passwords found across all [hashes.org "founds"](https://github.com/rarecoil/hashes.org-list) using Max Woolf's [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple). 

Note that GPT-2 was built for long-form text generation. This is an unorthodox use of this model, but as it is very popular, it was used for testing in order to publish some results in case others have considered using this widely-supported model for slow candidate generation.

## Experiment setup 

First, we generate the list used to train the model:

````bash
\$ head -n 10000000 hashes.org.final.txt > hashes.org.10m.txt
````

Finetuning was done on this list with 10,000 epochs on an AMD Radeon VII using the script below:

````python
import gpt_2_simple as gpt2

model_name = "124M"
gpt2.download_gpt2(model_name=model_name)

sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              'hashes.org.10m.txt',
              model_name=model_name,
              steps=10000)
````

After finetuning, the newly trained model was asked to generate passwords at a very high temperature of 1.2, in order to get extreme variance of the model and look for newer words. This temperature choice was arbitrary based on some basic sampling at lower temperatures; a more scientific approach for tuning temperature can be had here.

````python
import gpt_2_simple as gpt2

model_name = "124M"

for i in range(1000):
    with open('./gen-passwords.txt', 'a') as f:
        sess = gpt2.start_tf_sess()
        gpt2.load_gpt2(sess)
        for i in range(200):
            print("Generation %d" % i)
            data = gpt2.generate(sess,
                    temperature=1.2, # get weird.
                    batch_size=10,
                    nsamples=20,
                    return_as_list=True)
            f.write("\n".join(data))
        gpt2.reset_session(sess)
````

Note that after ever 200 sessions we reset using `gpt2.reset_session`. The rig containing the AMD Radeon VII, a Haswell i7-4790K with 32GB RAM, would exhaust memory doing generations over longer ranges. With this `batch_size` and `nsamples`, the Radeon generated approximately 111 passwords to the list per second.

Training was cut short after reaching in excess of 4 million password candidates.

## Results

After generation, the model had produced a total of 4,726,912 passwords. Using `sort -u` to receive a unique list, the model generated 4,053,784 unique passwords at this temperature. With this model, we can expect some overlap with our training set, so we can get the intersection of the lists:

````bash
\$ grep -Fx -f gen-passwords.unique.txt hashes.org.10m.txt | wc -l
822690
````

With 822,690 passwords generated directly existing in our training set, we have a unique password occurrence of 79.70% across our generated set. Using our 111 passwords/sec above, we get about 88 unique passwords per second from this model.

### Performance against popular password lists

Instead of running hashcat and using GPGPU power, we can use the entire hashes.org "founds" list of 822,898,440 passwords and check our intersection with the greater list using the same `grep` construct as above:

````bash
\$ grep -Fx -f gen-passwords.unique.txt hashes.org.final.txt | wc -l
1476624
````

Note that this list still contains the 822,690 duplicates, so the model generated `1476624 - 822690 =` 653,934 novel passwords that existed in the greater dataset, leading to some marginal success. 

## Conclusion

* 14.24% of all passwords generated over time had already been generated by the model (wasted computation)
* 17.4% of all passwords generated by the model are directly from its training set (wasted computation)
* 13.83% of all passwords generated by the model yield cracked passwords *not* in the training dataset (effectiveness)

Put another way, roughly 7 out of every 50 passwords generated by the model yield a crack, or **12.17 cracked passwords per second.**

## Appendices

`rawdata/` contains some of the data files used to produce this report. Please review and fix any inaccuracies.

* `generated.txt.gz` is the raw list that was originally `gen-passwords.txt` in the script above. It is all output from the GPT-2 model, with duplicates.
* `hashes.org.10m.txt.gz` is the raw list of 10M most common passwords across hashes.org founds as of November 2019. The full dataset used to validate results is too large for the repository, but may be found in [this repository](https://github.com/rarecoil/hashes.org-list).

The model generated is also too large for GitHub. You may download it as an XZ file [from this S3 link](https://s3.rarecoil.com/data/ai-passwords/gpt2-small.tar.xz) (approximately 437 MB). It contains the output of the `checkpoint/run1` folder from gpt-2-simple.